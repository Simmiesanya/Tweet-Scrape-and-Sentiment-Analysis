#import libraries

#data cleaning libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns
import string

import plotly.express as px #used to make express plots in plotly
import chart_studio.tools as cst #for exporting to chart studio
import chart_studio.plotly as py #for exporting plotly visuakizations for chart studio
import plotly.offline as pyo #set notebook mode to work in offline
pyo.init_notebook_mode()
import plotly.io as pio #plotly renderer
import plotly.graph_objects as go #for plotting plotly graph objects
from plotly.subplots import make_subplots

import configparser
from datetime import date
import re

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem.porter import *
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import words

#for sentiment analysis and machine learning
import transformers
from transformers import AutoModelForSequenceClassification
from transformers import TDNEpUTHQoQUJMHLrErGJyHg89uy71MyuHon
from transformers import AutoTokenizer
import numpy as np
from scipy.special import softmax # For out
import csv
import urllib.request
import torch
import nltk

#in order to produce customized wordcoud, install pillow
#using "pip install pillow" or "conda install -c anaconda pillow"

#download the stopwords list
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

#install the textblob library
#use pip install -U git+https://github.com/sloria/TextBlob.git@dev
#textblob is used to process textual data
import textblob
from textblob import TextBlob

import random #this is used to generate ranfom numbers
import os
#install the emoji module using the command line 
#then import the emoji module to enable you extract emoji from the text
import emoji

#install the clean-text module using the command line
#using pip install clean-text
from cleantext import clean
# Set the plot style to 'ggplot'
plt.style.use('ggplot')

# Read the CSV file into a DataFrame
df = pd.read_csv("C:\\Users\\OSANYA SIMI\\Documents\\My Bluetooth\\ThomasTuchelScrapeNEW.csv")

# Display the first 5 rows of the DataFrame
df.head(5)

#rename the columns
df.rename(columns={ df.columns[2]: "Username"}, inplace = True)
df.rename(columns={ df.columns[1]: "DateTime" }, inplace = True)
df.rename(columns={ df.columns[0]: "SN"}, inplace = True)
df.head(2)

#convert the DateTime column from object to Dataframe
df['DateTime'] = pd.to_datetime(df['DateTime'])
df.info()

#split the DateTime column into separate columns Date and Time
df['Date'] = pd.to_datetime(df['DateTime']).dt.date
df['Time'] = pd.to_datetime(df['DateTime']).dt.time
df.head(2)

#convert the date column to date format
df['Date'] = pd.to_datetime(df['Date'])
#drop all null values for the columns
df.dropna(subset=['Date', 'Time', 'Tweet'], inplace = True)

#drop the datetime column
df.drop("DateTime", axis=1, inplace=True)
df.head(2)

#rearrange the columns
df=df.iloc[:,[0,6,7,1,2,3,4,5]]
df.head(2)

df.rename(columns={ df.columns[7]: "Comments"}, inplace = True)
df.head(3)

#this will create a list of emojis and hen save them in the
#emoji_pattern dataframe
emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030"
                               "]+", flags=re.UNICODE)

#clean the data
import string
def cleanTxt(text):
    text = re.sub(r'@[A-Za-z0-9]+', '', text) #Remove @mentions replace with blank
    text = re.sub(r'#', '', text) #Remove the '#' symbol, replace with blank
    text = re.sub(r'RT[\s]+', '', text) #Removing RT, replace with blank
    text = re.sub(r'https?:\/\/\S+', '', text) #Remove the hyperlinks
    text = re.sub(r':', '', text) # Remove :
    text = re.sub(r'_', '', text) # Remove _
    text = re.sub(r'-', '', text) # Remove -
    text = re.sub(r'"', '', text) # Remove "
    text = text.translate(str.maketrans("","", string.punctuation))
    tweet_tokens = word_tokenize(text)
    filtered_words = [word for word in tweet_tokens if word not in stop_words]
    text = (emoji_pattern.sub(r'', text))
      return text
df['Tweet']= df['Tweet'].apply(cleanTxt)

# Convert the 'Tweet' column to lowercase
df['Tweet'] = df['Tweet'].str.lower()
# Display the DataFrame
display(df)

from transformers import AutoTokenizer, AutoModelForSequenceClassification
roberta = "cardiffnlp/twitter-roberta-base-sentiment"
# Initialize the tokenizer using the Roberta model
tokenizer = AutoTokenizer.from_pretrained(roberta)
# Initialize the model for sequence classification using the Roberta model
model = AutoModelForSequenceClassification.from_pretrained(roberta)

# Get an example tweet from the DataFrame
example = df['Tweet'][100001]
# Display the example tweet
example

#Define the encoded text
#this is the data you want to run sentiment analysis
# Encode the example text using the tokenizer
encoded_text=tokenizer(example, return_tensors='pt')

def sentiment_score(review):
    # Tokenize the review using the tokenizer
    tokens = tokenizer.encode(review, return_tensors='pt')
    # Get the sentiment score from the model output
    result = model(tokens) # Output
    # Return the predicted sentiment as an integer
    return int(torch.argmax(result.logits))
# Apply the sentiment_score function to each tweet in the 'Tweet' column of the DataFrame
df["Sentiment"] = df["Tweet"].apply(lambda x: sentiment_score(x[:512]))

def getInsight(score):
    # Determine the sentiment category based on the score
    if score == 0:
        return "Negative"
    elif score == 1:
        return "Neutral"
    else:
        return "Positive"
# Apply the getInsight function to each sentiment score in the 'Sentiment' column of the DataFrame
df["Insight"] = df["Sentiment"].apply(getInsight)

# Save the DataFrame to a CSV file
df.to_csv("C:\\Users\\OSANYA SIMI\\Documents\\My Bluetooth\\ThomasTuchelScrapeiSentiment.csv")
